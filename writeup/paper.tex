% TEMPLATE for Usenix papers, specifically to meet requirements of
%  USENIX '05
% originally a template for producing IEEE-format articles using LaTeX.
%   written by Matthew Ward, CS Department, Worcester Polytechnic Institute.
% adapted by David Beazley for his excellent SWIG paper in Proceedings,
%   Tcl 96
% turned into a smartass generic template by De Clarke, with thanks to
%   both the above pioneers
% use at your own risk.  Complaints to /dev/null.
% make it two column with no page numbering, default is 10 point

% Munged by Fred Douglis <douglis@research.att.com> 10/97 to separate
% the .sty file from the LaTeX source template, so that people can
% more easily include the .sty file into an existing document.  Also
% changed to more closely follow the style guidelines as represented
% by the Word sample file. 

% Note that since 2010, USENIX does not require endnotes. If you want
% foot of page notes, don't include the endnotes package in the 
% usepackage command, below.

% This version uses the latex2e styles, not the very ancient 2.09 stuff.
\documentclass[letterpaper,10pt]{article}
\usepackage{epsfig,graphicx,usenix,fullpage,float,hyperref}
\usepackage{mathtools,amssymb,setspace, pbox}
\usepackage[table,xcdraw]{xcolor}
\usepackage[center, labelfont=bf]{caption}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\def\bE{\mathbb{E}}

%\usepackage{endnotes}
\begin{document}


%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf 6.824 Final Project}
%for single author (just remove % characters)
\author{
{\rm Colleen Josephson}\\
cjoseph@mit.edu
\and
{\rm Joseph DelPreto}\\
delpreto@mit.edu
\and
{\rm Pranjal Vachaspati}\\
pranjal@mit.edu
\and
{\rm Steven Valdez}\\
dvorak42@mit.edu
% copy the following lines to add more authors
% \and
% {\rm Name}\\
%Name Institution
} % end author

\date{May 11, 2014}

\maketitle

% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
%\thispagestyle{empty}

\section{Introduction}
The presented project implements a persistent, fault-tolerant, high-performance key/value store. In order to achieve these goals, the system implements at-most-once semantics, relies upon Paxos for reaching consenus, shards the database across multiple replica groups, persistently stores values to disk, and implements protocols for automatically recovering and updating disk contents upon server reboot.  In order to improve performance, the system features two paxos optimizations: a protocol similar to multi-paxos to avoid the dueling leaders problem, and the ability for leaders to send out prepare messages in advance to reduce the required number of RPCs for paxos agreeement.  

Extensive testing was done to evaluate the system's persistence, recovery, and performance by extending existing tests and creating new ones.  After modifying the local RPC system to work on a real network, the system was deployed on an Amazon Web Services (AWS) cluster to obtain realistic measurements of throughput, latency, and disk recovery speed.

\section{Design} \label{sec:design}
The design is based upon the code developed throughout the semester for lab 4.  There are therefore three basic components running on each server: Paxos processes to reach consenus for instances, a shard master to determine which groups are responsible for various shards of the database, and a shard key/value storage process to execute client Put/Get requests.  The Paxos instances are used to create a consistent log of operations for both shard reconfiguration and client operations.  

Battery tests were run to determine which combination of our prior labs created the fastest and most reliable starting point.  Once a working codebase was compiled, modifications were made to address three  major areas: Paxos optimizations, persistence (with disk recovery), and testing over a real network on AWS.

\subsection{Paxos Optimizations}

\subsection{Persistent Storage}
In order to tolerate servers restarting and to handle databases that cannot fit in memory, all three components of the system store state and data persistently on disk.  To interface with the disk, a Go wrapper for the high-performance C++ key/value libray LevelDB was utilized.  An outline of what each system component stores on disk is presented below.
\begin{itemize}
\item The Paxos processes write the highest accepted value, the highest accepted proposal number, and the highest acknowledged prepare request to disk for each instance.  This ensures that the protocol will operate correctly even if a server restarts (for example, a decided value can never be changed).  In addition to this state, the Paxos processes persistently store the done responses received from peers and the number of the maximum known instance (see the recovery section below).

\item The shard master writes every configuration to disk, ensuring that will be able to respond to historical queries even after restarting.  In addition, it stores the highest executed Paxos log entry and the number of its highest known configuration. 

\item The shard key/value service writes its database to disk by writing each value as each Put request is executed.  In addition, it must preserve at-most-once semantics and consistency across reboots, so it persistently stores which operations have been seen as well as the most recent response sent to each client.  It also writes the highest executed Paxos log entry and the current configuration number.
\end{itemize}

In all cases, the specified data is written to disk before returning to the caller, so any response that a caller receives from a peer will not be forgotten by the responder even if it restarts.  It may be the case that the responder crashes before sending the response, but this is acceptable since the system offers at-most-once semantics (so the caller must be prepared to retry and the responder is prepared to filter duplicates).

\subsubsection{Recovery from Failures}
In order to tolerate servers restarting with or without their disk contents, the system includes protocols for recovering and catching up on startup.  These can be summarized by considering what happens when a server starts:

\begin{itemize}
\item The Paxos process will load any saved done responses and maximum instance number from its disk.   It will then contact peers to request their done responses and maximum instance number, determining which peer is the most updated out of any peers that respond.  From these values, both the ones read from disk and the ones received, it can determine which instances it should know about but does not; it then requests these instances from its peers.
\item The shard master process will load the highest known configuration and highest executed Paxos log instance from disk if it exists.  It will then request the same information from its peers, determining who among the responders is the most updated.  From this information it can determine if there are any missed configurations, and if so request them from a peer.  
\item The shard key/value service will load its last known configuration and the highest executed Paxos log instance from disk if it exists.  It will then request the same information from peers, determining the most updated state that it should have.  Finally, it can request the actual database key/value entries from a peer (see below for details of how this transfer is performed) along with any client responses and lists of seen operations.
\end{itemize}
These protocols allow any number of servers restarting with their disk contents to catch themselves up from any updated peer.  They also allow any number of servers restarting without their disk contents to recover the needed data from any peer that has the data.

\subsubsection{Memory and Performance Considerations}
There are a number of design choices which were made in order to address both memory management and overall performance.  

Firstly, the service must be able to handle database content which is too large to fit in volatile memory.  Towards this end, values are written to disk immediately as described above so it does not need to be kept in memory.  More importantly, however, servers that need to transfer shards (either because the shard master decided a shard needs to move or because a server is recovering) must be able to do so in multiple RPCs since memory is too small to create a reply with the entire database.  Therefore, a shard transfer occurrs with the following protocol:
\begin{itemize}
\item The requester sends a request to a peer, including a request number, and the current configuration number.  If doing a shard transfer, the shard number is also included.
\item If the receiver is currently at the appropriate configuration, it accepts the request.  It will then begin populating a reply with client response data, a list of seen operations, and the desired key/value data.  While doing this, it will periodically check memory usage; if it exceeds a configurable threshold, it will reply with whatever it currently has and mark the reply as incomplete.
\item The requester copies the data from the reply to its disk.  If the reply was marked as incomplete, it sends another request to the same peer with an incremented request number.
\item This continues until the reply is marked as complete, at which point the requester launches a goroutine to make sure the sender receives an acknowledgement of the receipt of the final data message.
\item If at any point the requester decides that the contacted peer is dead, it launches an acknowledgement goroutine as previously described for that peer (to terminate the transfer in case the peer wakes up or is not dead) and moves on to try a different peer, resetting the request number to 0.
\end{itemize}
Both the requester and the sender ignore client requests until the transfer is complete - this ensures that the requester will receive a consistent snapshot of the data (and is the reason the requester launches acknowledgement goroutines since otherwise a peer erroneously declared dead would be halted forever).

One aspect which addresses performance is that progress can still be made while recovery is taking place as long as a majority of servers are not recovering.  For example, if a replica group has three servers and a shard transfer is ocurring between two of them (perhaps because one just restarted), a client can still have its requests processed if it contacts the third peer.  This is because the Paxos service runs independently of the shard key/value service; although the latter will ignore client requests on the transferring servers, their Paxos processes can still respond and make progress.  Since the Paxos instances are stored persistently on disk, the two occupied servers can catch themselves up from the log when they complete the transfer.

In addition, effort was made to utilize memory in addition to the disk in order to increase performance.  The LevelDB database has the option to use a least-recently-used (LRU) cache to store recently used entries in memory.  In addition, a configuration variable was added to the service to allow values to be written to memory as well as disk until the memory is full.  These two options, combined with the native cache of the operating system, allow many needed values to be read quickly from memory rather than always consulting the disk.

When one server contacts another via the FetchRecovery RPC, both
servers enter a recovering state. This state means that neither are
allowed to modify their databases \emph{except} through the recovery
process. Depending on how many replicas there are, this may render a
replica group unable to process requests while recovery is underway.
If there are enough replicas so that paxos majority can still be
reached, then the recovering servers will catch up once they come back
online, using the functionality already built in to sync lagging paxos
servers.

All recovery processes send over both the relevant portions of the k/v
store, as well as paxos state and any state needed to detect
duplicates.

\textbf{Single server crashing and restarting, disk in-tact}: The
recovery process contacts a random replica in the same replica group.
The recovering server tells the up-to-date server its current state,
and the up-to-date server responds with any state the recoverer is
missing. This works because the server only discards old decisions if
all participants have marked that sequence number as 'done'.

\textbf{Single server crashing and disk lost}: The recovery process
contacts a random replica in the same replica group, and tells the
up-to-date server that it has no state. The two servers open an
auxillary TCP connection to send over the relevant portions of the
database. This avoids RPC overhead, since the database might be large.
The database has an in-order iterator, in case the TCP connection
fails, the recoverer can re-establish the connection and ask the
sender to pick up where it left off. If the sending server fails, then
the recovering server asks a new replica. This allows for a failure
mode where the original server remians locked permanently. We have
decided that this is preferable to sacrificing consistency, and the
system administartor can resolve the problem by rebooting the hing
server.

\textbf{Complete crash of all servers}: If all servers lose their disk
state, then all will restart and begin like a brand new deployment. If
the disks are not lost, then the recovery process will read the paxos
state written to disk and pick up where it left off. Because we write
to disk before any operation commits, data is not lost.

We use leveldb. To guarantee persistence, we enforce writing to disk
all operations before returning. This slows performance down, but
prevents loss of data that the client wants committed. Speed could be
gained using a Harp-like set of pointers, but this would be
complicated and the client would have to understand the modified
persistence model.

\subsection{Testing}
We added about 25 new tests, and extended existing ones to work with
persistent storage and run on TCP sockets instead of unix file
sockets. We use a few flags and configurations to run different modes
(e.g. with and without persistence, RPC using unix file sockets or
localhost TCP).

Below we describe the different tests. All tests are located at
'mexos/src/$<$name$>$/test\_test.go' unless otherwise noted.

\emph{paxos:} Paxos specific tests

\emph{shardmaster:} Shardmaster specific tests

\emph{shardkv:} Shardkv specific tests

\emph{test:} Tests and benchmarks modified to run on our AWS
cluster. These tests are the most complicated, and require the user to
ssh into our cluster leader and start the servers on other cluster
machines using 'start-on-all.sh' to run
'mexos/src/main/start.go'. These tests required a surprising amount of
debugging and glue code. Additionally, to check for correct operation
under server failure, we had to make a backdoor RPC that told the
remote server to act deaf or dead.

We present the results of our benchmarks in a later section.

\section{Performance}

We created an extensive system to try and accurately measure
performance. In addition to writing many benchmarks, we also modified
the original RPC system so that we could run the servers in an AWS
cluster and observe the performance on a real network.

\subsection{Throughput and Latency}

We compared performance between vanilla paxos and multipaxos under a
variety of conditions. The tests were run on an actual network in an
AWS cluster. 

\textit{Client latency}: Average time it takes for one client to see a
request processed. 

\textit{Throughput}: Number of requests, on average, the
client will see processed in a second.

\textit{System throughput}: Number of requests per second the entire
system can handle. This is different than the client throughput
because clients always wait to see the response of the current request
before dispatching another.

System throughput requires a saturation of multiple clients sending
continuous requests over a ten second period. The system saturated
from 10 to 50 clients, sepending on the number of
groups/replicas. After the saturation point, adding more clients
causes a decrease in throughput occurs because of network traffic
saturation.

We also ran some tests repeatedly on one key, and others on a random
set of keys. This allows us to look at how the performance changes
when requests all go to the same replica group and when they are
evenly spread across all replica groups (assuming the staff-provided
key2shard isn't biased).

% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass ``xcolor=table'' option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[h]
\begin{tabular}{|l|l|l|l|}
\hline
\multicolumn{4}{|c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \textbf{Vanilla Paxos}}}                           \\ \hline
                              & \textbf{Avg. Client Latency} & \textbf{Avg. Client Thruput} & \textbf{System Thruput} \\ \hline
\multicolumn{4}{|c|}{\textit{PutHash to one key}}                                                                     \\ \hline
\textbf{1 group, 3 replicas}  & 40.3 ms/op                   & 24.8 requests/s              & 27 requests/s           \\ \hline
\textbf{1 group, 10 replicas} & 5370 ms/op                   & 0.18 requests/s              & 2 requests/s            \\ \hline
\textbf{3 groups, 3 replicas} & 62 ms/op                     &                              & 22 requests/s           \\ \hline
\multicolumn{4}{|c|}{\textit{PutHash to many keys}}                                                                   \\ \hline
\textbf{1 group, 3 replicas}  & 40.7 ms/op                   & 24.5 requests/s              & 30 requests/s           \\ \hline
\textbf{1 group, 10 replicas} & 282 ms/op                    & 3.5 requests/s               & 7 requests/s            \\ \hline
\textbf{3 groups, 3 replicas} & 214 ms/op                    & 4.6 requests/s               & 67 requests/s           \\ \hline
\end{tabular}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass ``xcolor=table'' option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[h]
\begin{tabular}{|l|l|l|l|}
\hline
\multicolumn{4}{|c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \textbf{Multipaxos}}}                              \\ \hline
                              & \textbf{Avg. Client Latency} & \textbf{Avg. Client Thruput} & \textbf{System Thruput} \\ \hline
\multicolumn{4}{|c|}{\textit{PutHash to one key}}                                                                     \\ \hline
\textbf{1 group, 3 replicas}  & 27 ms/op                     & 37 requests/s                & 47 requests/s           \\ \hline
\textbf{1 group, 10 replicas} & 2360 ms/op                   & 0.42 requests/s              & 3.4 requests/s          \\ \hline
\textbf{3 groups, 3 replicas} & 22 ms/op                     & 45 requests/s                & 45 requests/s           \\ \hline
\multicolumn{4}{|c|}{\textit{PutHash to many keys}}                                                                   \\ \hline
\textbf{1 group, 3 replicas}  & 25 ms/op                     & 40 requests/s                & 45 requests/s           \\ \hline
\textbf{1 group, 10 replicas} & 130 ms/op                    & 7 requests/s                 & 10.3 requests/s         \\ \hline
\textbf{3 groups, 3 replicas} & 40 ms/op                     & 25 requests/s                & 124 requests/s          \\ \hline
\end{tabular}
\end{table}

For a single group case, the difference between three replicas and ten is
striking. The more replicas, the more traffic is needed for a paxos
agreement. Figure 1 demonstrates how throughput suffers as the number
of replicas increases. One other interesting effect is that as the
number of replicas increases, the larger the gap there is between the
single key and multi-key cases. Because there is one group, all puts
touch the same servers. We are unsure what causes this.

Adding more groups increases throughput, however. For evenly
distributed traffic, going from one group to 3 nearly triples the
throughput. Figure 2 shows that this trend continues, so adding groups
linearly improves the throughput of the system. The table shows that
average throughput for a single client may suffer, but this is
probably because the system throughput benchmark creates many clients,
while the client throughput test creates a single client. Having
multiple clients increases the liklihood that a client must wait to
see its request served, since the replicas may be busy serving another
client's request.

\begin{figure}[H]
\centering
\makebox[\textwidth][c]{
\includegraphics[scale=.50]{replicas.png}}
\caption{Performance of a one-group configuration rapidly deteriorates as the number of replicas increases}
\label{fig:replicas}
\end{figure}

\begin{figure}[H]
\centering
\makebox[\textwidth][c]{
\includegraphics[scale=.5]{groups.png}}
\caption{Performance increases linearly as the number of groups increases. There is probably a saturation point, but we didn't have enough cluster machines to reach it.}
\label{fig:groups}
\end{figure}

\subsubsection{Paxos RPC Counts}

Multipaxos decreases the number of RPCs needed to reach agreement.

% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass ``xcolor=table'' option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[H]
\centering
\makebox[\textwidth][c]{
\begin{tabular}{|l|l|l|}
\hline
\multicolumn{3}{|c|}{\cellcolor[HTML]{C0C0C0}\textbf{Paxos RPC Counts}}                  \\ \hline
                                 & \textbf{Single Proposer} & \textbf{Multiple Proposer} \\ \hline
\textbf{Normal Paxos}            & 30                       & 89            \\ \hline
\textbf{Paxos with Leaders}      & 20                       & 27                         \\ \hline
\textbf{Leaders and Pre-prepare} & 17                       & 20                         \\ \hline
\end{tabular}
}
\end{table}

\subsubsection{Disk Recovery}
Raw time to write 1GB to database, HDD:  21.470750409s

Raw time to write 1GB to database, SSD:  6.1470524260s

Recovery time of a 1GB database using our system: TBD

\subsection{Bottlenecks}
Although we made significant improvements performance-wise, we
identified a number of bottlenecks.

One notable bottleneck is disk writes. The database library we used,
levedb, uses a cache and is quite fast. However, in the process of
debugging networked RPCs, we implemented simple logging. This logging
slowed down the operations by nearly a factor of ten. This was a good
demonstration of how care must be taken in even the most mundane
aspects, such as logging.

Another bottleneck is network capacity and/or number of messages. As
shown in the latency and thoughput section, the increasing the number
of replicas has an adverse effect on performance since each operation
requires communication with a majority of replicas. 

Increasing the number of replica groups increases the performance of
the system for normal operation, especially under heavy load with
evenly distributed key requests. This is especially noticable in the
system throughput metric.  The number of groups roughly corresponds to
how parallelizable the operations are.

Using multipaxos reduces the amount of network communication required,
which improves the throughput and latency by about a factor of
two. Multipaxos also reduces the amount of paxos state information,
since prepares are pre-determined. This saves some disk space.  We
also suspect that using UDP would also improve throughput, as servers
that do not recieve a response already re-transmit their request,
making the TCP ack system redundant.

\section{Conclusion}
This project implements, tests, and benchmarks a persistent and
performant sharded key/value store. While it is not the next Spanner,
the end result is reasonable considering the time and manpower
limitations.

%{\footnotesize \bibliographystyle{acm}
%\bibliography{paper}}
%\newpage
\end{document}
